{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "0Jx0qceO_-sm"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqF6juxSNwMd"
      },
      "source": [
        "# Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Arr2AeN3gl"
      },
      "source": [
        "Contiguous memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJrQqGAHBQm_",
        "outputId": "bc4c5495-44fe-4ce4-97ad-b54b962f2c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.7576, 0.2793, 0.4031],\n",
            "         [0.7347, 0.0293, 0.7999],\n",
            "         [0.3971, 0.7544, 0.5695],\n",
            "         [0.4388, 0.6387, 0.5247]],\n",
            "\n",
            "        [[0.6826, 0.3051, 0.4635],\n",
            "         [0.4550, 0.5725, 0.4980],\n",
            "         [0.9371, 0.6556, 0.3138],\n",
            "         [0.1980, 0.4162, 0.2843]]])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "tensor([0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544, 0.5695,\n",
            "        0.4388, 0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550, 0.5725, 0.4980,\n",
            "        0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Shape: torch.Size([2, 4, 3])\n",
            "Stride: (12, 3, 1)\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(1)\n",
        "x = torch.rand((2, 4, 3), dtype=torch.float32)\n",
        "\n",
        "\n",
        "print(x)\n",
        "print(\"-\"*100)\n",
        "print(x.flatten())\n",
        "print(\"-\"*100)\n",
        "print(\"Shape:\", x.shape)\n",
        "print(\"Stride:\", x.stride())\n",
        "print(x.is_contiguous())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CecfkP35V-kN"
      },
      "source": [
        "View vs copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvpbDevgWBKy",
        "outputId": "b4027954-97af-4daa-f202-37fd22b9c481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3])\n",
            "(3, 1)\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "y_view = x[0]\n",
        "print(y_view.shape)\n",
        "print(y_view.stride())\n",
        "print(y_view.is_contiguous())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD9e3KJmVs8P"
      },
      "source": [
        "Not all views are contiguous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vZyOEhoHLKr",
        "outputId": "27020fe1-494d-4187-f95f-0e69390a6c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.7576, 0.7347, 0.3971, 0.4388],\n",
            "         [0.2793, 0.0293, 0.7544, 0.6387],\n",
            "         [0.4031, 0.7999, 0.5695, 0.5247]],\n",
            "\n",
            "        [[0.6826, 0.4550, 0.9371, 0.1980],\n",
            "         [0.3051, 0.5725, 0.6556, 0.4162],\n",
            "         [0.4635, 0.4980, 0.3138, 0.2843]]])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "torch.Size([2, 3, 4])\n",
            "(12, 1, 3)\n",
            "False\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(1)\n",
        "x = torch.rand((2, 4, 3), dtype=torch.float32)\n",
        "\n",
        "y = x.transpose(1, 2)\n",
        "\n",
        "print(y)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "print(y.shape)\n",
        "print(y.stride())\n",
        "print(y.is_contiguous())\n",
        "print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIRd_ehDHs_q",
        "outputId": "5153f6f0-3dfc-4a21-a9b3-6291acbf5432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "(3, 1)\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "y = x[:,0].clone()\n",
        "print(y.shape)\n",
        "print(y.stride())\n",
        "print(y.is_contiguous())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8xvG_PJKKl-"
      },
      "source": [
        ".view(), .expand(), .unsqueeze()/.squeeze(), .transpose(), .permute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qpgcIRyMg6h",
        "outputId": "2fdf806a-94a4-4ad5-9b31-eafdf208c9fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 3])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.random.manual_seed(1)\n",
        "x = torch.randn(2, 4, 3)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP2Ia-0lSd75",
        "outputId": "001210c3-4085-41cd-8997-5c12129f53ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3])\n",
            "True\n",
            "(3, 1)\n"
          ]
        }
      ],
      "source": [
        "y_reshape = x.view(8, 3)\n",
        "print(y_reshape.shape)\n",
        "print(y_reshape.is_contiguous())\n",
        "print(y_reshape.stride())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvKLTN0bmx-H",
        "outputId": "30b6631e-906e-4282-a10f-924d17d96421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
          ]
        }
      ],
      "source": [
        "y = x.transpose(0,1)\n",
        "print(y.is_contiguous())\n",
        "\n",
        "try:\n",
        "  y.view(8, 3)\n",
        "except RuntimeError as e:\n",
        "  print(f\"RuntimeError: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCoLVPJGnn_V",
        "outputId": "41169b93-0b50-4527-b90a-b84848590763"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 3])"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.random.manual_seed(1)\n",
        "x = torch.randn(2, 4, 3)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-thlb6-D3ir",
        "outputId": "18bc6661-5368-49bf-8a87-d4e666d5100b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 4, 3])\n",
            "torch.Size([2, 1000000000, 4, 3])\n",
            "(12, 0, 3, 1)\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "y_exp = x.unsqueeze(1)\n",
        "print(y_exp.shape)\n",
        "y_exp = y_exp.expand(2, 1_000_000_000, 4, 3)\n",
        "print(y_exp.shape)\n",
        "print(y_exp.stride())\n",
        "print(y_exp.is_contiguous())\n",
        "# print(y_exp.stride())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv6dfSw1UJV8"
      },
      "source": [
        ".view() vs .reshape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfyVlIlOUN8v",
        "outputId": "b8f4cb12-ef2a-4fd1-9d82-a6a251e1c56b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    y_exp.view(2_000_000_000, 4, 3)\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "jH_8uMcgUomb"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     y_exp.reshape(2_000_000_000, 4,3)\n",
        "# except RuntimeError as e:\n",
        "#     print(f\"RuntimeError: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7BPAMZNQcaw"
      },
      "source": [
        "cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UneDRFIBfTX",
        "outputId": "c89d0c81-9c77-4e8c-ebc5-e3cc723e9c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZYoiPLRqoTS",
        "outputId": "78b748a6-01bd-48c9-8292-96f57c3f98d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device == 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxxsqiScB1IN",
        "outputId": "f806cda6-c663-4071-8d75-f2dce32afbc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU time: 0.08591103553771973\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "x = torch.randn(5000, 5000)\n",
        "start = time.time()\n",
        "y = x @ x\n",
        "\n",
        "\n",
        "print(\"CPU time:\", time.time() - start)\n",
        "\n",
        "if device == 'cuda':\n",
        "    x = x.to(device)\n",
        "    start = time.time()\n",
        "    y = x @ x\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"GPU time:\", time.time() - start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxO23MQTYCHu"
      },
      "source": [
        "- torch.nn.Parameter: model-tracked tensor with gradients (used in all neural nets)\n",
        "- TensorDict: batched dictionary of tensors (ideal for structured data, RL, dynamic models)\n",
        "- LazyTensor: symbolic tensor used in PyTorch 2.x compiler stack (torch.compile, tracing)\n",
        "- PackedSequence: handles variable-length sequences in RNNs (efficient batching)\n",
        "- Meta tensor: zero-allocation tensor for shape checking or debugging models without memory usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqKfUejTYFXM"
      },
      "source": [
        "# Automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFHB73bkIOBL"
      },
      "source": [
        "Let $f_i(z_{i-1}; w_{i})$ be some function for $i \\in [n].$ Define\n",
        "$$\n",
        "L(x;w) = f_n(f_{n-1}(\\cdots f_1(x; w_1) \\cdots; w_{n-1}); w_n)\n",
        "$$\n",
        "We want to compute the derivatives $\\frac{\\partial L}{\\partial w_i}(x,w),$ $i = 1,\\dots,n$.\n",
        "\n",
        "Define intermediate variables:\n",
        "$$\n",
        "z_0 = x, \\quad z_i = f_i(z_{i-1}; w_i) \\text{ for } i = 1, \\ldots, n, \\quad L = z_n.\n",
        "$$\n",
        "\n",
        "Backward propagation computes:\n",
        "$$\n",
        "\\delta_n = \\frac{\\partial L}{\\partial z_n} = 1\n",
        "$$\n",
        "\n",
        "and for $i = n,\\dots, 1$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_i} = \\delta_i \\cdot \\frac{\\partial f_i}{\\partial w_i}(z_{i-1},w_i).\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\delta_{i-1} = \\delta_i \\cdot \\frac{\\partial f_i}{\\partial z_{i-1}}(z_{i-1},w_i),\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaOMVgbVYc-g",
        "outputId": "bb663b60-a90a-4906-dd3a-ef14a7491249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dz/dx = tensor([0.4308, 0.2670, 0.3022])\n",
            "softmax tensor([0.4308, 0.2670, 0.3022], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(1)\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "\n",
        "loss = torch.logsumexp(x, dim = 0)\n",
        "\n",
        "loss.backward()\n",
        "print(\"dz/dx =\", x.grad)\n",
        "print('softmax', torch.softmax(x, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ase6PKGA4e9c"
      },
      "source": [
        "Computational graph, .is_leaf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp9e1I2RtiZi"
      },
      "source": [
        "$z =((x *y) + sin(x))^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvyE_iM5rFhK",
        "outputId": "14930296-8583-4fd3-eda3-ecfef19a91f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dL/dx = tensor(35.7052)\n",
            "dL/dy = tensor(27.6372)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# Intermediate values\n",
        "a = x * y\n",
        "b = torch.sin(x)\n",
        "c = a + b\n",
        "z = c**2\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print(\"dL/dx =\", x.grad)\n",
        "print(\"dL/dy =\", y.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNsjh0Jkt9pD",
        "outputId": "d3cfab68-310a-48d1-fb3f-3d80650a41ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PowBackward0 at 0x12f980df0>"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.grad_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HB8QPnUqAw0",
        "outputId": "3848a61b-006d-4d1b-ee69-e07622361763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PowBackward0\n",
            "   AddBackward0\n",
            "     MulBackward0\n",
            "       AccumulateGrad\n",
            "       AccumulateGrad\n",
            "     SinBackward0\n",
            "       AccumulateGrad\n"
          ]
        }
      ],
      "source": [
        "def print_graph(fn, indent=0):\n",
        "    if fn is None:\n",
        "        return\n",
        "    print(\" \" * indent, f\"{type(fn).__name__}\")\n",
        "    for next_fn, _ in fn.next_functions:\n",
        "        print_graph(next_fn, indent + 2)\n",
        "\n",
        "print_graph(z.grad_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "uP8ScZmaZ0jc"
      },
      "outputs": [],
      "source": [
        "#!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "FvjIbhNAZ22Z",
        "outputId": "1663d4a4-609b-4740-cfd9-08188459c94f"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torchviz import make_dot\n",
        "# from IPython.display import Image\n",
        "\n",
        "# # Step 4: Visualize the graph\n",
        "# dot = make_dot(z, params={'x': x, 'y': y, 'a': a, 'b': b, 'c': c, 'z': z})\n",
        "# dot.format = \"png\"\n",
        "# dot.render(\"comp_graph\", cleanup=True)\n",
        "# Image(\"comp_graph.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJat-auF09Mx"
      },
      "source": [
        "Gradients accumulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQK6hWEczhgL",
        "outputId": "9a96e989-1d28-4141-84f3-d39c0dc9aab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ After first backward:\n",
            "x.grad = tensor([0.3760, 0.2959, 0.3281])\n",
            "\n",
            "⛔ After second backward (without zeroing):\n",
            "x.grad = tensor([1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(342)\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "\n",
        "# First backward\n",
        "loss1 = torch.logsumexp(x, dim= 0)\n",
        "loss1.backward()\n",
        "\n",
        "print(\"✅ After first backward:\")\n",
        "print(\"x.grad =\", x.grad)\n",
        "\n",
        "# Second backward without zeroing the gradient\n",
        "loss2 = x.sum()\n",
        "x.grad.zero_()\n",
        "# loss2 = torch.logsumexp(x, dim= 0)\n",
        "loss2.backward()\n",
        "\n",
        "print(\"\\n⛔ After second backward (without zeroing):\")\n",
        "print(\"x.grad =\", x.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYh7puAvxWdm"
      },
      "source": [
        "jacobian, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS9FlQeunDsa",
        "outputId": "8ca9c4dc-15af-4ccd-fc11-44f0cdb3d53d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jacobian:\n",
            "tensor([[ 0.2452, -0.1150, -0.1302],\n",
            "        [-0.1150,  0.1957, -0.0807],\n",
            "        [-0.1302, -0.0807,  0.2109]])\n",
            "\n",
            "Gradient:\n",
            "tensor([0.4308, 0.2670, 0.3022])\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd.functional import jacobian\n",
        "from torch.autograd import grad\n",
        "\n",
        "torch.manual_seed(1)\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "\n",
        "def f(x):\n",
        "    return torch.softmax(x, 0)\n",
        "\n",
        "J = jacobian(f, x)\n",
        "print(\"Jacobian:\")\n",
        "print(J)\n",
        "\n",
        "def g(x):\n",
        "  return torch.logsumexp(x, 0)\n",
        "\n",
        "\n",
        "print(\"\\nGradient:\")\n",
        "print(grad(g(x), x)[0])\n",
        "# print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV2K6L4P6Kah",
        "outputId": "fef89b66-6e4a-4e39-ed9f-55cac09f0a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ First: Compute gradient with autograd.grad\n",
            "grad(y, x): tensor([0.4308, 0.2670, 0.3022])\n",
            "\n",
            "⛔ Then: Try backward() on the same graph\n",
            "RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
            "\n",
            "✅ Finally: Recompute y and call backward() again\n",
            "x.grad: tensor([0.4308, 0.2670, 0.3022])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "\n",
        "y = torch.logsumexp(x, 0)\n",
        "print(\"✅ First: Compute gradient with autograd.grad\")\n",
        "print(\"grad(y, x):\", grad(y, x)[0])\n",
        "\n",
        "\n",
        "print(\"\\n⛔ Then: Try backward() on the same graph\")\n",
        "try:\n",
        "    y.backward()\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n✅ Finally: Recompute y and call backward() again\")\n",
        "y = torch.logsumexp(x, 0)\n",
        "y.backward()\n",
        "print(\"x.grad:\", x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6urQyDt2xklY",
        "outputId": "9d7640f2-91e3-479b-9859-75f99c6c21bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(12.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**3\n",
        "dy_dx = grad(y, x, create_graph=True)[0]\n",
        "d2y_dx2 = grad(dy_dx, x)[0]\n",
        "print(d2y_dx2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1n3V6yKZD-r"
      },
      "source": [
        "Clones share the computational graph with the original variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKAiBXOBY-BK",
        "outputId": "13336ba5-7db2-4ed5-fb04-d558384eab2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.grad: tensor([0.5320, 0.0212, 0.5792, 0.2876, 0.5462])\n",
            "x_copy.is_leaf: False\n",
            "x_copy.grad: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/sh/7ch7dbpn0db2smxkj3s5hbf80000gn/T/ipykernel_20429/3066404.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
            "  print(\"x_copy.grad:\", x_copy.grad)\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, requires_grad=True)\n",
        "\n",
        "x_copy = x.clone()  #.detach().requires_grad_() # Not detached!\n",
        "y = (x_copy).norm()\n",
        "y.backward()\n",
        "\n",
        "print(\"x.grad:\", x.grad)\n",
        "print(\"x_copy.is_leaf:\", x_copy.is_leaf)\n",
        "print(\"x_copy.grad:\", x_copy.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSU9lSZNUTrn",
        "outputId": "5eb1d746-02f2-4ba7-aa79-51d6212fa7d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n",
            "Gradient at x:  None\n",
            "----------------------------------------------------------------------------------------------------\n",
            "x requires grad?:  True\n",
            "x is a leaf tensor?:  False\n",
            "tensor([[0.2887, 0.2647, 0.3015, 0.2805],\n",
            "        [0.3095, 0.2401, 0.2692, 0.2677],\n",
            "        [0.2893, 0.2756, 0.3564, 0.3046]])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "tensor([[0.2887, 0.2647, 0.3015, 0.2805],\n",
            "        [0.3095, 0.2401, 0.2692, 0.2677],\n",
            "        [0.2893, 0.2756, 0.3564, 0.3046]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand((3,4), requires_grad= True)\n",
        "x_copy = x.clone().detach().requires_grad_()\n",
        "\n",
        "\n",
        "# ❌ PyTorch throws here because in-place ops on leaf tensors that require gradients are dangerous. If it allowed this, it would overwrite values that autograd needs.\n",
        "try:\n",
        "  x += 1\n",
        "except RuntimeError as e:\n",
        "  print(f\"RuntimeError: {e}\")\n",
        "\n",
        "print('Gradient at x: ', x.grad)\n",
        "print(\"-\"*100)\n",
        "\n",
        "# ✅ This is NOT in-place. But now... x is no longer a leaf tensor.\n",
        "x = x + 1\n",
        "print('x requires grad?: ', x.requires_grad)\n",
        "print('x is a leaf tensor?: ', x.is_leaf)\n",
        "\n",
        "x.retain_grad()\n",
        "\n",
        "y = x.norm()\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "\n",
        "print(\"-\"*100)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  x_copy += 1\n",
        "y = x_copy.norm()\n",
        "y.backward()\n",
        "\n",
        "print(x_copy.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2GxC3nmSpDj",
        "outputId": "3982566b-237b-457d-a53e-04103a011cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  x = torch.rand((3,4), requires_grad= True)\n",
        "  # y = x[:2]\n",
        "  # y += 1\n",
        "  x += 1\n",
        "except RuntimeError as e:\n",
        "  print(f\"RuntimeError: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtWtyd598eaH"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqDXXhzZ76dk"
      },
      "outputs": [],
      "source": [
        "num_agents = 1000\n",
        "num_items = 2\n",
        "num_features = 3\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "x_i_j_k = torch.normal(0,1,(num_agents, num_items, num_features))\n",
        "beta0_k = torch.randint(-10,10, size = (num_features,), dtype= x_i_j_k.dtype)\n",
        "# beta0_k = torch.ones(num_features, dtype=x_i_j_k.dtype) \n",
        "\n",
        "U_i_j = x_i_j_k @ beta0_k\n",
        "mu_i_j = torch.softmax(U_i_j, dim= 1)\n",
        "j_i = torch.multinomial(mu_i_j, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIro0j_U0V5E",
        "outputId": "fba8af17-56b1-4625-9f79-a207c532b60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0005)\n"
          ]
        }
      ],
      "source": [
        "Lip_ll =  2 * x_i_j_k.norm(dim= 2).max(1).values.sum()\n",
        "\n",
        "learning_rate = 1.9 / Lip_ll\n",
        "print(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPlZEaGazYJb"
      },
      "source": [
        "$$- ll = - \\log \\prod_{i \\ in I} \\frac{\\exp(X_{i j_i}^\\top \\beta)}{\\sum_j \\exp(X_{i j}^\\top \\beta)} = \\sum_i \\log \\sum_j \\exp(X_{i j}^\\top \\beta)  - \\sum_i X_{i j_i}^\\top \\beta$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "rYyV4RIVz3gw"
      },
      "outputs": [],
      "source": [
        "def neg_log_likelihood(beta_k, x_i_j_k, j_i):\n",
        "\n",
        "    U_i_j = x_i_j_k @ beta_k\n",
        "    U_i_hat = U_i_j.gather(1, j_i)\n",
        "\n",
        "    return torch.logsumexp(U_i_j, dim=1).sum() - U_i_hat.sum()\n",
        "\n",
        "def compute_MLE_diy(x_i_j_k, j_i, tol=1e-4, lr=1e-2, num_epochs = 1000):\n",
        "\n",
        "    _, _, num_features = x_i_j_k.shape\n",
        "\n",
        "    torch.manual_seed(2)\n",
        "    beta_k = torch.randn(num_features, requires_grad=True)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if beta_k.grad is not None:\n",
        "            beta_k.grad.zero_()\n",
        "\n",
        "        loss = neg_log_likelihood(beta_k, x_i_j_k, j_i)\n",
        "        loss.backward()\n",
        "\n",
        "        grad_norm = beta_k.grad.norm()\n",
        "        if grad_norm < tol:\n",
        "            print(f\"Converged at tolerance:\", tol)\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            beta_k -= lr * beta_k.grad\n",
        "\n",
        "    return beta_k\n",
        "\n",
        "def compute_MLE(x_i_j_k, j_i, tol = 1e-4, lr = 1e-2, num_epochs = 1000):\n",
        "\n",
        "    _,_,num_features = x_i_j_k.shape\n",
        "    beta_k = torch.randn(num_features, requires_grad=True)\n",
        "\n",
        "    optimizer = torch.optim.SGD([beta_k], lr= lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        loss = neg_log_likelihood(beta_k, x_i_j_k, j_i)\n",
        "        loss.backward()\n",
        "\n",
        "        if beta_k.grad.norm() < tol:\n",
        "            print(f\"Converged at tolerance:\", tol)\n",
        "            break\n",
        "        optimizer.step()\n",
        "\n",
        "    return beta_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_lqGExP4MvU",
        "outputId": "0cb97e2f-59b2-4d89-b946-76c445d5553b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 3.5353, -0.2296, -8.6008], requires_grad=True)\n",
            "tensor([ 3.,  0., -7.])\n",
            "Time: 0.4663581848144531\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5000\n",
        "lr = learning_rate\n",
        "\n",
        "tic = time.time()\n",
        "beta_hat = compute_MLE(x_i_j_k, j_i, lr= lr, num_epochs = num_epochs)\n",
        "print(beta_hat)\n",
        "print(beta0_k)\n",
        "print(\"Time:\", time.time() - tic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMK2mCvPSNNi",
        "outputId": "2a352b58-2f94-4ca4-c197-9868478a29ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asymptotic variance:\n",
            "tensor([[ 0.1541, -0.0083, -0.3318],\n",
            "        [-0.0083,  0.0179,  0.0198],\n",
            "        [-0.3318,  0.0198,  0.7962]])\n",
            "Standard errors: tensor([0.3926, 0.1336, 0.8923])\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd.functional import hessian\n",
        "\n",
        "H_k_k = hessian(lambda beta_k: neg_log_likelihood(beta_k, x_i_j_k, j_i), beta_hat)\n",
        "Avar_k_k = torch.linalg.inv(H_k_k)\n",
        "print(\"Asymptotic variance:\")\n",
        "print(Avar_k_k)\n",
        "print(\"Standard errors:\", torch.sqrt(torch.diag(Avar_k_k)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iQ0rfdqpsgG"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "JW9MF25UpuYy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "num_agents = 1000\n",
        "num_items = 10\n",
        "num_features = 3\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "x_i_j_k = torch.normal(0,1,(num_agents, num_items, num_features))\n",
        "beta0_k = torch.randint(-10,10, size = (num_features,), dtype= x_i_j_k.dtype)\n",
        "\n",
        "U_i_j = x_i_j_k @ beta0_k\n",
        "mu_i_j = torch.softmax(U_i_j, dim = 1)\n",
        "j_i = torch.multinomial(mu_i_j, 1)\n",
        "\n",
        "dataset = TensorDataset(x_i_j_k, j_i)\n",
        "# loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "x_train, j_train = dataset[train_dataset.indices]\n",
        "x_test, j_test = dataset[test_dataset.indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCoG5yRR4Mqn",
        "outputId": "12b43303-e426-4bb1-bd9f-42a5325746d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-8.1355,  7.0501,  0.1358], requires_grad=True)\n",
            "tensor([-8.,  7.,  0.])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LogitNet(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.logit_model = nn.Linear(num_features, 1, bias=False)\n",
        "\n",
        "    def forward(self, x_i_j_k):\n",
        "        I, J, K = x_i_j_k.shape\n",
        "        x_ij_k = x_i_j_k.view(I * J, K)\n",
        "\n",
        "        U_i_j = self.logit_model(x_ij_k).view(I, J)\n",
        "        return U_i_j\n",
        "\n",
        "logit_example = LogitNet(num_features)\n",
        "optimizer = torch.optim.SGD(logit_example.parameters(), lr=1e-2)\n",
        "torch.manual_seed(2)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    U_i_j = logit_example(x_train)\n",
        "    loss = loss_fn(U_i_j, j_train.squeeze(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "if num_features< 5:\n",
        "    with torch.no_grad():\n",
        "        beta_net = logit_example.logit_model.weight.squeeze()\n",
        "        print(beta_net)\n",
        "        print(beta0_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "xb4yqNk4ZCyh"
      },
      "outputs": [],
      "source": [
        "class LogitNet3Layer(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.output = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, x_i_j_k):\n",
        "        I, J, K = x_i_j_k.shape\n",
        "        x = x_i_j_k.view(I * J, K)\n",
        "\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        U_i_j = self.output(x).view(I, J)\n",
        "\n",
        "        return U_i_j\n",
        "\n",
        "\n",
        "nn_example = LogitNet3Layer(num_features, hidden_dim = 64)\n",
        "optimizer = torch.optim.SGD(nn_example.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    U_i_j = nn_example(x_train)\n",
        "    loss = loss_fn(U_i_j, j_train.squeeze(1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(nn_example.parameters(), max_norm=1.0)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cGZItfcfbNR",
        "outputId": "a8cdf5db-7dc8-46c8-9cf3-83134973ee3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogitNet  - Train Acc: 0.897, Test Acc: 0.915\n",
            "3LayerNet - Train Acc: 0.919, Test Acc: 0.900\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(model, x_train, j_train):\n",
        "  model.eval()\n",
        "  logits = model(x_train)\n",
        "  return (logits.argmax(dim=1) == j_train.squeeze(1)).float().mean()\n",
        "\n",
        "\n",
        "acc_logit_train = compute_accuracy(logit_example, x_train, j_train)\n",
        "acc_logit_test = compute_accuracy(logit_example, x_test, j_test)\n",
        "\n",
        "acc_nn_train = compute_accuracy(nn_example, x_train, j_train)\n",
        "acc_nn_test = compute_accuracy(nn_example, x_test, j_test)\n",
        "\n",
        "print(f\"LogitNet  - Train Acc: {acc_logit_train:.3f}, Test Acc: {acc_logit_test:.3f}\")\n",
        "print(f\"3LayerNet - Train Acc: {acc_nn_train:.3f}, Test Acc: {acc_nn_test:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
